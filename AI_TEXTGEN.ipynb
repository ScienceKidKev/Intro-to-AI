{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "ai=aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The US Department of Defense has issued an order for the Pentagon to make sure that the US military does not use drones in combat during the 2018 military campaign.\n",
      "\n",
      "The order also requires that the Pentagon provide \"full and complete information on the use of such unmanned aerial vehicles in hostilities against the US in combat situations in Iraq and Afghanistan.\"\n",
      "\n",
      "\"It is not necessary to disclose any classified information that is not pertinent to the operation or to the ongoing operations in Iraq and Afghanistan,\" the order reads. \"Further, no information or information that is relevant to the operation or to the current operations in Iraq and Afghanistan is to be divulged except to the extent that the information is essential for the operational or strategic purposes of the United States. The information may be classified at any time.\"\n",
      "\n",
      "The order also requires that the Pentagon \"ensure that the following information is not disseminated about any of the operations in Iraq and Afghanistan: the number of personnel who have been killed or injured, the number of casualties or civilian casualties, the number of personnel killed or wounded, the number of injured or killed in the line of fire, or the number of persons killed or injured.\"\n",
      "\n",
      "The order does not apply to the US Coast Guard, which is conducting an investigation into the incident\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI really like unicorns they are so\u001b[0m great.\n",
      "\n",
      "I'm very interested in the nature of the story, you can see where they are coming from and what they do with that information.\n",
      "\n",
      "What do you make of the idea that they may be hiding in that same area?\n",
      "\n",
      "I think they're the biggest and most important part of the story.\n",
      "\n",
      "Do you believe that they are the most important part of the story and that they are the most important part of the history?\n",
      "\n",
      "I think so. I think they are the most important part of the story, but that they are the most important parts of the history.\n",
      "\n",
      "What do you think of the idea that they are the most important part of the story and that they are the most important part of the history?\n",
      "\n",
      "I think those things would make good stories.\n",
      "\n",
      "I really like what you have to say about the history of the Native Americans.\n",
      "\n",
      "I have a great story with the Native Americans of the world.\n",
      "\n",
      "I love the history of their people.\n",
      "\n",
      "Can you tell me what other cultures you have explored in your life?\n",
      "\n",
      "I love everything about my life.\n",
      "\n",
      "I love all of the cultures I have explored.\n",
      "\n",
      "I\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m I decided to use one for my wedding dress. It's a bit more light to the touch and I love the color and it's a nice touch to the dress. I wish they would have made a couple of different colors for this to look like.\n",
      "\n",
      "Rated 5 out of 5 by jockeys from Good for the price I had been wanting to get a dress for this wedding. I have been using the pomfered version for years and when I saw the pomfered version I was excited. I had to order this one. I love the color and that it's simple, but also it's very comfortable and it's really flattering. I had to get the pomfered version because it was a bit thick and I wanted a different look. I love the fact that it's so easy to cut the neckline off. Also, the dress has the option to make it a full lace dress but it's a bit big for my stomach so I would not recommend it. The only thing is, the dress is so big you can't take it off.\n",
      "\n",
      "Rated 5 out of 5 by russianfrom It's not the prettiest dress I've ever worn I wear this dress every\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m cute but I am very nervous about the one being too tall. I am 4'5\". I am pretty confident in my height but I am very nervous about the two being too short.\n",
      "\n",
      "\n",
      "I think I am getting more comfortable with the fact that I am 6'3\" and I would like to have a bigger chest but I am not afraid to get my chest size up to 6\"\n",
      "\n",
      "\n",
      "I am 6'3\" and I do not know if I will be able to fit into my short torso at the start, I have also heard that a 6'1\" guy might want to know if he is taller than 6'4\" I am 6'4\" and I do not know if I will be able to fit into my short torso at the start, I have also heard that a 6'1\" guy might want to know if he is taller than 6'4\"\n",
      "\n",
      "Sterling\n",
      "\n",
      "Senior Member\n",
      "\n",
      "Registered: October, 2015 Posts: 4,739\n",
      "\n",
      "Rep: 8\n",
      "\n",
      "Location: Los Angeles, CA, USA\n",
      "\n",
      "\n",
      "I am 6'4\" and my mom is 5'4\" and my brother is 5'11\". I am tall for my\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='I really like unicorns they are so', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mArtificial Intelligence is a technology \u001b[0m that attempts to build the next level of AI. This is a fundamental part of the technology. It's not the only thing that can be done to make AI work. It's something you can also do to enhance it.\n",
      "But the technology is also about changing the way we think. And this is what's unique about artificial intelligence. We don't just think of it as a tool; we have to think about it as a system that can be used to make intelligent systems more effective.\n",
      "I think the reason that we do that is because there's so much more that can be done. We're just beginning to create a system that could be used by a lot of different people.\n",
      "Why do you think that's important?\n",
      "There are many different reasons:\n",
      "1. It's more likely that we'll change the way we think. We think more about what we are looking at as a system. It's more likely that we'll change the way we think about things.\n",
      "2. It's more likely that we'll change the way we think about things. Our understanding of how things work has changed and we're starting to see things that are not what we expect.\n",
      "3. It's more likely that we\n",
      "==========\n",
      "\u001b[1mArtificial Intelligence is a technology \u001b[0m that can be used to build a computer program that can solve problems. It has this potential to be a better and more creative means of solving problems. It is the hope that it could be used to solve problems in a way that is more efficient and flexible than any other new technology. It is the hope that it could solve problems in an innovative way that is more efficient than any other new technology. \n",
      "The world is changing. \n",
      "The world is changing. The world is changing.\n",
      "Let's start with the basic concepts \n",
      "The world is changing. The world is changing.\n",
      "Let's start with the basic concepts. \n",
      "This is the basic concept of the world.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "This is the basic design of the world.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "This is the basic design of the world.\n",
      "The world will change.\n",
      "The world will change.\n",
      "The world will change.\n",
      "This is the basic\n",
      "==========\n",
      "\u001b[1mArtificial Intelligence is a technology \u001b[0m that has been around for almost a century.\n",
      "It is based on a combination of Artificial Intelligence, Artificial Intelligence, Machine Learning, Computer Vision, Artificial Intelligence. It is also based on the ability to learn from experience (like the human brain from a computer).\n",
      "In a nutshell, it is a new form of technology.\n",
      "How it works:\n",
      "The Artificial Intelligence is the ability of a computer to learn from a human experience or learn something from the computer.\n",
      "The Machine Learning is the ability of a computer to learn a human experience or learn something from the computer.\n",
      "The Computer Vision is the ability of a computer to learn from a computer's visual experience.\n",
      "By working on the machine learning, the computer learns something from its experience and the computer learns something from it.\n",
      "By working on the computer vision, the computer learns something from its experience and the computer learns something from it.\n",
      "By working on the machine learning, the computer learns something from the experience and the computer learns something from it.\n",
      "By working on the machine learning, the computer learns something from the experience and the computer learns something from it.\n",
      "By working on the machine learning, the computer learns something from the experience and the computer learns something from it\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='Artificial Intelligence is a technology ', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWhat is sentience? \u001b[0m It is the ability to move into another's body, or one's own, with the intention of experiencing the world in a different way.  It is the ability to be emotionally open, to understand and express oneself in a manner that is unafraid of rejection, and to be able to see the world in a way that is not afraid of rejection.  It is the ability to be open, to express oneself in a manner that is unafraid of rejection, and to be able to see the world in a way that is not afraid of the rejection of the person from whom you came.  It is the ability to be open with empathy, with the ability to be open with love, with the ability to be open with faith, with the ability to be open with the gift of the Spirit of our Creator.  It is the ability to be open with humility, with the ability to be open with wisdom, with the ability to be open with goodness, with the ability to be open with love, with the ability to be open with the gift of the Spirit of our Creator.  It is the ability to be open with compassion, with the ability to be open with compassion, with the ability to\n",
      "==========\n",
      "\u001b[1mWhat is sentience? \u001b[0m When I first learned it, I was curious.  I was excited about learning the word, but I did not know what the meaning was.  I never really bothered to ask what it was.  I never really had any idea, but it turned out that it's a combination of the two.  I could tell that it was a combination of the two, as I tried to pick out what I thought of it.  I also knew it was a word that I was curious about and wanted a better understanding of.  I asked myself the question of what it means and who it is.  I wanted to understand, and I wanted to try to decipher what it means.  I got a few clues, but I wasn't sure what to do with them.  What I did know was that it means a feeling of empathy and love.  I had already spent a lot of time in the school, but I was always curious and curious about what it meant.  I didn't need to learn it myself.  I wanted to try to find out what it meant, and I didn't want to take it away from anyone.  I just wanted to\n",
      "==========\n",
      "\u001b[1mWhat is sentience? \u001b[0m It's pretty simple, and the answer is that it takes time to learn.  This is why we spend our time in the field of social interaction (i.e., the human face, the hand, etc.).  It takes time to learn that we're not just human, but we're all human too.  And that's what makes this field so fascinating.  So, what do we learn about ourselves, our bodies, and our relationships when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we're not interacting with each other?  Well, what do we learn about ourselves when we\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='What is sentience? ', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAre you a AI? \u001b[0m (This is an opinion I'm in no doubt about.  It's a fact that AI is a bit of a beast.  I'm not sure I can make any conclusions, but I do know that there is a vast amount more to human psychology than we've ever explored before.)\n",
      "==========\n",
      "\u001b[1mAre you a AI? \u001b[0m Do you have any questions about this post?  I'd love to hear them in the comments.\n",
      "Thank you for your support!\n",
      "==========\n",
      "\u001b[1mAre you a AI? \u001b[0m Are you trying to make any AI as smart as you can?  Do you have any ideas?  What would you like to see improved over time?  What would you like the best AI to do?  Do you want to see an AI that can learn and understand human language?  Do you want to see a AI that can understand human language?  Do you want to have AI that can communicate with a human?  Do you want to have AI that can tell if humans are there or not?  Do you want to have AI that can understand human language?  Do you want to have AI that can tell if humans are there or not?  Do you want to have AI that can learn and understand human language?  Do you want to have AI that can tell if human languages are human?  Do you want to have AI that can learn and understand human language?  Do you want to have AI that can tell if human languages are human?  Do you want to have AI that can learn and understand human language?  Do you want to have AI that can communicate with human language?  Do you want to have AI that can\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='Are you a AI? ', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('SHAKESPERE.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 90479.11it/s]\n"
     ]
    }
   ],
   "source": [
    "config=GPT2ConfigCPU()\n",
    "\n",
    "ai=aitextgen(tokenizer_file='aitextgen.tokenizer.json', config=config)\n",
    "\n",
    "data=TokenDataset('SHAKESPERE.txt', tokenizer_file='aitextgen.tokenizer.json', block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:466: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m                  \n",
      "Loss: 3.430 — Avg: 3.452: 100%|██████████| 5000/5000 [04:20<00:00, 19.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m                        \n",
      "Loss: 3.430 — Avg: 3.452: 100%|██████████| 5000/5000 [04:20<00:00, 19.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========                                                                   \n",
      " o'en, in Londita,                                                           \n",
      "Are fore thanks from his groanks,\n",
      "Is glory of thy friends, and therefore,\n",
      "And what, so doubted: a ben till we\n",
      "Aremen tall\n",
      "==========                                                                   \n",
      "Loss: 3.430 — Avg: 3.452: 100%|██████████| 5000/5000 [04:20<00:00, 19.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.430 — Avg: 3.452: 100%|██████████| 5000/5000 [04:20<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "You must be somet, for I cannot be here,\n",
      "Thank'd forgotion to seek the house.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "K:\n",
      "My lord, I amongue me, and you might winhave\n",
      "Offior,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "She'st thou be ill?\n",
      "\n",
      "Prayor:\n",
      "In mock me infess and sovereme to be gone!\n",
      "\n",
      "MILLIUS:\n",
      "Master, sir, I'll fellow:\n",
      "I have too so.\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, and therefore he were it backlcome,\n",
      "Against to be strong by lower on and timp\n",
      "I chroose, and dwomanle: by the royaltune\n",
      "And take my father's love?\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "We may find I must be want,\n",
      "Tellow that shall do the bune.\n",
      "\n",
      "KING RICHARD II:\n",
      "So forthumbune a mock,\n",
      "Inton, but hair, in this corrue,\n",
      "Anardon\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Hath she charge in the glory inning,\n",
      "Thy else of the business: and you have been\n",
      "That mock'd them: if you have a full,\n",
      "To be fulleed.\n",
      "\n",
      "SICINIUS:\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Either, I do; and, as we are\n",
      "O, sir, I may come, and, as he lives me here.\n",
      "\n",
      "GRUMIO:\n",
      "You can not.\n",
      "\n",
      "KATHARINA:\n",
      "All, one, if you am as.\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Willst thou shalt?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Third Watchman:\n",
      "Farewell, if thou not be glork in this.\n",
      "\n",
      "BALTHATER:\n",
      "Ay, cousiness,\n",
      "He\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Sir, and not doubt though you have done to be a day.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I'll be so so rool.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Come, to my lord, and you?\n",
      "\n",
      "PRO\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I'll confess you to ceue to the faint\n",
      "theerest and till he dishonests, in the stuffs\n",
      "And yet I not bride to give him last:\n",
      "And I am the revenge that I to call\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "But, come, with melay.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I fear you sit us.\n",
      "\n",
      "GREEN:\n",
      "Tweet, that is the mind:\n",
      "Now say I'll go with agin the lawful\n",
      "With fie\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt='ROMEO:')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
